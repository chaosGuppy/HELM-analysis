{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load import load_tasks_data, load_model_task_data\n",
    "import numpy as np\n",
    "from difficulty import get_difficulty_per_trial, quantize_difficulties, convert_difficulties_to_quantiles\n",
    "from accuracy import get_accuracy_per_trial, get_accuracy_per_model, Split, normalize_accuracy\n",
    "import json\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.othermod.betareg import BetaModel\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = load_tasks_data()\n",
    "TASK = \"boolq\"\n",
    "accuracy_per_model = get_accuracy_per_model(TASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_per_trial = get_accuracy_per_trial(accuracy_per_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_difficulties = get_difficulty_per_trial(accuracy_per_trial, exclude_models=[], num_options=tasks[TASK].get(\"num_options\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_models = [\"openai_ada\", \"openai_curie\", \"openai_babbage\", \"ai21_j1-large\", \"together_gpt-j-6b\", \"together_gpt-neox-20b\", \"together_opt-66b\", \"microsoft_TNLGv2_7B\" ]\n",
    "test_models = [\"openai_davinci\", \"ai21_j1-jumbo\", \"together_bloom\", \"together_opt-175b\",  \"microsoft_TNLGv2_530B\", \"together_yalm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models.json\") as f:\n",
    "    param_counts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_with_difficulty(instance_difficulties, accuracy_per_trial, param_counts, models):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for trial_id, difficulty in instance_difficulties.items():\n",
    "        for model in models:\n",
    "            features.append([difficulty, np.log(param_counts[model])])\n",
    "            labels.append([record for record in accuracy_per_trial[trial_id] if record['model'] == model][0][\"is_correct\"])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_level_accuracy_per_model(accuracy_per_model, models, num_options=None):\n",
    "    accuracy_per_model = {model: sum([record[\"is_correct\"] for record in accuracy_per_model[model]]) / len(accuracy_per_model[model])\n",
    "        for model in models}\n",
    "    if num_options is not None:\n",
    "        accuracy_per_model = {model: normalize_accuracy(accuracy_per_model[model], num_options) for model in models}\n",
    "    return accuracy_per_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(accuracy_per_model, param_counts, models):\n",
    "    mean_accuracy_per_model = get_task_level_accuracy_per_model(accuracy_per_model, models)\n",
    "    features = []\n",
    "    labels = []\n",
    "    for model in models:\n",
    "        features.append([np.log(param_counts[model])])\n",
    "        labels.append(mean_accuracy_per_model[model])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_task_level_accuracy(clf, param_count, instance_difficulties, num_options=None):\n",
    "    instance_level_predictions = clf.predict_proba(\n",
    "            [[difficulty, np.log(param_count)]\n",
    "            for difficulty in instance_difficulties.values()]\n",
    "        )[:, 1]\n",
    "    accuracy = np.mean(instance_level_predictions)\n",
    "    if num_options is not None:\n",
    "        accuracy = normalize_accuracy(accuracy, num_options)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_level_predictions_instance(clf, models, param_counts, instance_difficulties, num_options=None):\n",
    "    return {model: predict_task_level_accuracy(clf, param_counts[model], instance_difficulties, num_options) for model in models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_beta_model(features, labels):\n",
    "    model = BetaModel(np.array(labels), np.array(features))\n",
    "    r = model.fit()\n",
    "    return model, r.params\n",
    "\n",
    "def predict_beta_model(model, params, features):\n",
    "    return model.predict(params, exog=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def group_instances_by_difficulty(instance_difficulties, num_bins=10):\n",
    "    bin_edges = np.linspace(0, 1, num_bins)\n",
    "    bins = np.digitize(np.array(list(instance_difficulties.values())), bin_edges)\n",
    "    instance_ids_by_difficulty = defaultdict(list)\n",
    "    for instance_id, bin in zip(instance_difficulties.keys(), bins):\n",
    "        instance_ids_by_difficulty[bin].append(instance_id)\n",
    "    return instance_ids_by_difficulty\n",
    "\n",
    "def get_accuracy_per_difficulty(model, instance_ids_by_difficulty, accuracy_per_trial):\n",
    "    accuracy_per_difficulty = {}\n",
    "    for difficulty, instance_ids in instance_ids_by_difficulty.items():\n",
    "        accuracy_per_difficulty[difficulty] = np.mean([record[\"is_correct\"] for instance_id in instance_ids for record in accuracy_per_trial[instance_id] if record[\"model\"] == model])\n",
    "    return accuracy_per_difficulty\n",
    "\n",
    "\n",
    "def predict_task_level_accuracy_with_binned_difficulties(model, params, param_count, instance_difficulties, num_options=None):\n",
    "    binned_difficulties = group_instances_by_difficulty(instance_difficulties)\n",
    "    instance_level_predictions = predict_beta_model(model, params,\n",
    "            [[difficulty, np.log(param_count)]\n",
    "            for difficulty in binned_difficulties.keys()]\n",
    "        )\n",
    "    accuracy = np.average(instance_level_predictions, weights=[len(ids) for ids in binned_difficulties.values()])\n",
    "    if num_options is not None:\n",
    "        accuracy = normalize_accuracy(accuracy, num_options)\n",
    "    return accuracy\n",
    "\n",
    "def get_task_level_predictions_binned(model, params, models, param_counts, instance_difficulties, num_options=None):\n",
    "    return {model_name: predict_task_level_accuracy_with_binned_difficulties(model, params, param_counts[model_name], instance_difficulties, num_options) for model_name in models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_with_binned_difficulty(instance_difficulties, accuracy_per_trial, param_counts, models):\n",
    "    features = []\n",
    "    labels = []\n",
    "    instances_by_difficulty = group_instances_by_difficulty(instance_difficulties, 10)\n",
    "    for model in models:\n",
    "        accuracy_per_difficulty = get_accuracy_per_difficulty(model, instances_by_difficulty, accuracy_per_trial)\n",
    "        for difficulty, accuracy in accuracy_per_difficulty.items():\n",
    "            features.append([difficulty, np.log(param_counts[model])])\n",
    "            accuracy = min(accuracy, 1 - 1e-6)\n",
    "            accuracy = max(accuracy, 1e-6)\n",
    "            labels.append(accuracy)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_level_predictions(clf, models, param_counts):\n",
    "    return {model: clf.predict([[np.log(param_counts[model])]])[0] for model in models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_accs(clf, models, param_counts, num_options=None):\n",
    "    difficulties = np.linspace(0, 1, 100)\n",
    "    for model in models:\n",
    "        features = [[difficulty, np.log(param_counts[model])] for difficulty in difficulties ]\n",
    "        predictions = clf.predict_proba(features)[:, 1]\n",
    "        plt.plot(difficulties, predictions, label=model)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_difficulty_based_approach(instance_difficulties, accuracy_per_model, accuracy_per_trial, task):\n",
    "    train_features, train_labels = featurize_with_difficulty(instance_difficulties, accuracy_per_trial, param_counts, train_models)\n",
    "    test_features, test_labels = featurize_with_difficulty(instance_difficulties, accuracy_per_trial, param_counts, test_models)\n",
    "    print(len(train_features))\n",
    "    clf = LogisticRegression().fit(train_features, train_labels)\n",
    "    acc = get_task_level_accuracy_per_model(accuracy_per_model, test_models)\n",
    "    prd = get_task_level_predictions_instance(clf, test_models, param_counts, instance_difficulties, num_options=tasks[task].get(\"num_options\"))\n",
    "    return mean_squared_error(list(acc.values()), list(prd.values()))\n",
    "\n",
    "def score_task_level_approach(accuracy_per_model):\n",
    "    train_features, train_labels = featurize(accuracy_per_model, param_counts, train_models)\n",
    "    test_features, test_labels = featurize(accuracy_per_model, param_counts, test_models)\n",
    "    clf = LinearRegression().fit(train_features, train_labels)\n",
    "    acc = get_task_level_accuracy_per_model(accuracy_per_model, test_models)\n",
    "    prd = get_task_level_predictions(clf, test_models, param_counts)\n",
    "    return mean_squared_error(list(acc.values()), list(prd.values()))\n",
    "\n",
    "def score_quantized_difficulty_based_approach(instance_difficulties, accuracy_per_trial, task):\n",
    "    difficulty_quantiles = convert_difficulties_to_quantiles(instance_difficulties)\n",
    "    instance_difficulties = {instance_id: quantile for instance_id, quantile in zip(instance_difficulties.keys(), difficulty_quantiles)}\n",
    "    train_features, train_labels = featurize_with_binned_difficulty(instance_difficulties, accuracy_per_trial, param_counts, train_models)\n",
    "    test_features, test_labels = featurize_with_binned_difficulty(instance_difficulties, accuracy_per_trial, param_counts, test_models)\n",
    "    model, params = fit_beta_model(train_features, train_labels)\n",
    "    acc = get_task_level_accuracy_per_model(accuracy_per_model, test_models)\n",
    "    prd = get_task_level_predictions_binned(model, params, test_models, param_counts, instance_difficulties, num_options=tasks[task].get(\"num_options\"))\n",
    "    return mean_squared_error(list(acc.values()), list(prd.values()))\n",
    "\n",
    "def compare_approaches(task: str):\n",
    "    accuracy_per_model = get_accuracy_per_model(task)\n",
    "    accuracy_per_trial = get_accuracy_per_trial(accuracy_per_model)\n",
    "    instance_difficulties = get_difficulty_per_trial(accuracy_per_trial, exclude_models=[], num_options=tasks[task].get(\"num_options\"))\n",
    "    difficulty_based_score = score_difficulty_based_approach(instance_difficulties, accuracy_per_model, accuracy_per_trial, task)\n",
    "    task_level_score = score_task_level_approach(accuracy_per_model)\n",
    "    binned_difficulty_score = score_quantized_difficulty_based_approach(instance_difficulties, accuracy_per_trial, task)\n",
    "    return difficulty_based_score, task_level_score, binned_difficulty_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_approaches(\"synthetic_reasoning_pattern_match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HELM-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "043970097948fba43b83f281cf920e467929c79aa6be48203f54b9a4c9a540ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
